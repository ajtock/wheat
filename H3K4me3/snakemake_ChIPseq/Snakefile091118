# Snakemake workflow for mapping ChIP-seq libraries to a reference genome
# Adapted from https://github.com/seb-mueller/snakemake_sRNAseq

# Chromosome sizes file below ("data/index/genome.fa.sizes") must exist
# before running snakefile
# e.g., in "data/index/" run:
# samtools faidx genome.fa; cut -f1,2 genome.fa.fai > genome.fa.sizes

# Usage (snakemake --cores should reflect available cores):
# conda env create --file environment.yaml --name ChIPseq_mapping
# source activate srna_mapping
# snakemake -p --cores 48
# source deactivate

import pandas as pd
import os

# To make the per_base_coverage rule work with a shell script invoked using the "shell" directive,
# we need to determine the base path of Snakefile since we expect the scripts directory to be there as well
SRCDIR = srcdir("")

# Specify config file parameters
configfile: "config.yaml"
reference  = config["MAPPING"]["reference"]
refbase    = os.path.basename(reference)

# Determine bam index format (bai or csi) based on chromosome sizes
# Genomes with chromosomes longer than ~500 Mb (e.g., in wheat) require a csi index
# E.g., in axolotl: https://sourceforge.net/p/samtools/mailman/message/36249039/
chrSizes = pd.read_table("data/index/" + refbase + ".fa.sizes",
                         header = None)
smallChrs = 0
for x in chrSizes[1]:
    if x < 5e+08:
        smallChrs = smallChrs + 1

if smallChrs < len(chrSizes[1]):
    bamidx = "csi"
else:
    bamidx = "bai"

# Get sample names from samples.csv
samples = pd.read_table("samples.csv",
                        header = 0,
                        sep = ",",
                        index_col = 0)

# Specify the desired end target file(s)
rule all:
    input:
        #expand("logs/fastqc/raw/{sample}_R1_fastqc.html",
        #       sample = samples.index),
        #expand("logs/fastqc/raw/{sample}_R2_fastqc.html",
        #       sample = samples.index),
        ###expand("data/{sample}_R1_R2_interleaved.fastq.gz",
        ###       sample = samples.index),
        ###expand("data/dedup/{sample}_R1_R2_interleaved_dedup.fastq.gz",
        ###       sample = samples.index),
        #expand("data/dedup/{sample}_R1_dedup.fastq.gz",
        #       sample = samples.index),
        #expand("data/dedup/{sample}_R2_dedup.fastq.gz",
        #       sample = samples.index),
        #expand("data/dedup/{sample}_R1_dedup_repair.fastq.gz",
        #       sample = samples.index),
        #expand("data/dedup/{sample}_R2_dedup_repair.fastq.gz",
        #       sample = samples.index),
        #expand("data/dedup/trimmed/{sample}_R1_dedup_repair_trimmed.fastq.gz",
        #       sample = samples.index),
        #expand("data/dedup/trimmed/{sample}_R2_dedup_repair_trimmed.fastq.gz",
        #       sample = samples.index),
        #expand("logs/fastqc/trimmed/{sample}_R1_dedup_repair_trimmed_fastqc.html",
        #       sample = samples.index),
        #expand("logs/fastqc/trimmed/{sample}_R2_dedup_repair_trimmed_fastqc.html",
        #       sample = samples.index),
        #expand("mapped/{sample}_MappedOn_{refbase}_sort.bam",
        #       sample = samples.index,
        #       refbase = refbase),
        expand("mapped/{sample}_MappedOn_{refbase}_sort_lowXM.sam",
               sample = samples.index,
               refbase = refbase),
        #expand("mapped/{sample}_MappedOn_{refbase}_sort.bam.{bamidx}",
        #       sample = samples.index,
        #       refbase = refbase,
        #       mode = mode,
        #       bamidx = bamidx),
        #expand("mapped/bg/{sample}_MappedOn_{refbase}_sort_norm.bedgraph",
        #       sample = samples.index,
        #       refbase = refbase,
        #       mode = mode),
        #expand("mapped/pb/{sample}_MappedOn_{refbase}_sort_norm.perbase",
        #       sample = samples.index,
        #       refbase = refbase,
        #       mode = mode)

# Run fastqc on R1 raw data
rule fastqc_R1_raw:
    """Create fastqc report"""
    input:
        "data/{sample}_R1.fastq.gz"
    output:
        html = "logs/fastqc/raw/{sample}_R1_fastqc.html",
        zip  = "logs/fastqc/raw/{sample}_R1_fastqc.zip"
    params: "--extract"
    log:
        "logs/fastqc/raw/{sample}_R1.log"
    wrapper:
        "0.27.1/bio/fastqc"
# Run fastqc on R2 raw data
rule fastqc_R2_raw:
    """Create fastqc report"""
    input:
        "data/{sample}_R2.fastq.gz"
    output:
        html = "logs/fastqc/raw/{sample}_R2_fastqc.html",
        zip  = "logs/fastqc/raw/{sample}_R2_fastqc.zip"
    params: "--extract"
    log:
        "logs/fastqc/raw/{sample}_R2.log"
    wrapper:
        "0.27.1/bio/fastqc"

### "This program ran out of memory. [>330G! IMPRACTICAL FOR WHEAT]"  
### "Try increasing the -Xmx flag and using tool-specific memory-related parameters."
## 1. Created single interleaved paired-end reads file
## 2. Deduplicate interleaved reads
## 3. Deinterleave deduplicated file
#rule reformat_dedup:
#    """Interleave paired-end reads, deduplicate and deinterleave"""
#    input:
#        fastq1 = "data/{sample}_R1.fastq.gz",
#        fastq2 = "data/{sample}_R2.fastq.gz"
#    output:
##        inter  = "data/{sample}_R1_R2_interleaved.fastq.gz",
##        dedup  = "data/dedup/{sample}_R1_R2_interleaved_dedup.fastq.gz",
#        fastq1 = "data/dedup/{sample}_R1_dedup.fastq.gz",
#        fastq2 = "data/dedup/{sample}_R2_dedup.fastq.gz"
#    threads: config["THREADS"]
#    log:
#        "logs/reformat_dedup/{sample}_reformat_dedup.log"
#    shell:
#        "reformat.sh int=f vpair=t in1={input.fastq1} in2={input.fastq2} out=stdout.fastq "
#        "| dedupe.sh interleaved=t ac=f threads={threads} in=stdin.fastq out=stdout.fastq "
#        "| reformat.sh int=t vint=t vpair=t in=stdin.fastq out1={output.fastq1} out2={output.fastq2} 2> {log}"

# Deduplicate R1 reads
rule dedupe_R1:
    """Remove duplicate R1 reads"""
    input:
        "data/{sample}_R1.fastq.gz"
    output:
        "data/dedup/{sample}_R1_dedup.fastq.gz"
    threads: config["THREADS"]
    params:
        memory = config["MEMORY"]
    log:
        "logs/dedup/{sample}_R1_dedup.log"
    shell:
        "(dedupe.sh -Xmx{params.memory} in={input} out={output} threads={threads} ac=f) 2> {log}"
# Deduplicate R2 reads
rule dedupe_R2:
    """Remove duplicate R2 reads"""
    input:
        "data/{sample}_R2.fastq.gz"
    output:
        "data/dedup/{sample}_R2_dedup.fastq.gz"
    threads: config["THREADS"]
    params:
        memory = config["MEMORY"]
    log:
        "logs/dedup/{sample}_R2_dedup.log"
    shell:
        "(dedupe.sh -Xmx{params.memory} in={input} out={output} threads={threads} ac=f) 2> {log}"

# Re-pair separately deduplicated reads
rule repair:
    """Re-pair separately deduplicated reads"""
    input:
        fastq1 = "data/dedup/{sample}_R1_dedup.fastq.gz",
        fastq2 = "data/dedup/{sample}_R2_dedup.fastq.gz"
    output:
        fastq1 = "data/dedup/{sample}_R1_dedup_repair.fastq.gz",
        fastq2 = "data/dedup/{sample}_R2_dedup_repair.fastq.gz",
        fastq3 = "data/dedup/{sample}_dedup_singletons.fastq.gz"
    params:
        memory = config["MEMORY"]
    log:
        "logs/repair/{sample}_R1_R2_dedup_repair.log"
    shell:
        "(repair.sh -Xmx{params.memory} repair=t"
        " in1={input.fastq1} in2={input.fastq2}"
        " out1={output.fastq1} out2={output.fastq2}"
        " outs={output.fastq3}) 2> {log}"

# Trim off adapters
rule cutadapt:
    """Remove adapters"""
    input:
        "data/dedup/{sample}_R1_dedup_repair.fastq.gz",
        "data/dedup/{sample}_R2_dedup_repair.fastq.gz"
    output:
        fastq1 = "data/dedup/trimmed/{sample}_R1_dedup_repair_trimmed.fastq.gz",
        fastq2 = "data/dedup/trimmed/{sample}_R2_dedup_repair_trimmed.fastq.gz",
        qc     = "data/dedup/trimmed/{sample}_dedup_repair_trimmed.qc.txt"
    params:
        " -u " + str(config["FILTER"]["cutadapt"]["R1_5prime_cut"]) +
        " -u " + str(config["FILTER"]["cutadapt"]["R1_3prime_cut"]) +
        " -U " + str(config["FILTER"]["cutadapt"]["R2_5prime_cut"]) +
        " -U " + str(config["FILTER"]["cutadapt"]["R2_3prime_cut"]) +
        " -a " +     config["FILTER"]["cutadapt"]["adapter_R1"] +
        " -A " +     config["FILTER"]["cutadapt"]["adapter_R2"] +
        " -O " + str(config["FILTER"]["cutadapt"]["minimum-overlap"]) +
        " -q " + str(config["FILTER"]["cutadapt"]["quality-filter"]) +
        " -m " + str(config["FILTER"]["cutadapt"]["minimum-length"]) +
        " -M " + str(config["FILTER"]["cutadapt"]["maximum-length"]) +
        " --cores=0"
    log:
        "logs/cutadapt/{sample}_dedup_repair_trimmed.log"
    wrapper:
        "0.27.1/bio/cutadapt/pe"

# Run fastqc on R1 trimmed data
rule fastqc_R1_trimmed:
    """Create fastqc report"""
    input:
        "data/dedup/trimmed/{sample}_R1_dedup_repair_trimmed.fastq.gz"
    output:
        html = "logs/fastqc/trimmed/{sample}_R1_dedup_repair_trimmed_fastqc.html",
        zip  = "logs/fastqc/trimmed/{sample}_R1_dedup_repair_trimmed_fastqc.zip"
    params: "--extract"
    log:
        "logs/fastqc/trimmed/{sample}_R1_dedup_repair_trimmed.log"
    wrapper:
        "0.27.1/bio/fastqc"
# Run fastqc on R2 trimmed data
rule fastqc_R2_trimmed:
    """Create fastqc report"""
    input:
        "data/dedup/trimmed/{sample}_R2_dedup_repair_trimmed.fastq.gz"
    output:
        html = "logs/fastqc/trimmed/{sample}_R2_dedup_repair_trimmed_fastqc.html",
        zip  = "logs/fastqc/trimmed/{sample}_R2_dedup_repair_trimmed_fastqc.zip"
    params: "--extract"
    log:
        "logs/fastqc/trimmed/{sample}_R2_dedup_repair_trimmed.log"
    wrapper:
        "0.27.1/bio/fastqc"

# Align to reference genome
rule bowtie2:
    """Map reads using bowtie2 and sort them using samtools"""
    input:
        fastq1 = "data/dedup/trimmed/{sample}_R1_dedup_repair_trimmed.fastq.gz",
        fastq2 = "data/dedup/trimmed/{sample}_R2_dedup_repair_trimmed.fastq.gz"
    output:
        "mapped/{sample}_MappedOn_{refbase}_sort.bam"
    log:
        "logs/bowtie2/{sample}_MappedOn_{refbase}_sort.log"
    params:
        alignments = config["MAPPING"]["alignments"]
    threads: config["THREADS"]
    shell:
        "(bowtie2 --very-sensitive --no-mixed --no-discordant"
        " --threads {threads} -k {params.alignments}"
        " -x {reference} -1 {input.fastq1} -2 {input.fastq2} "
        "| samtools view -u -f 1 -F 12 - "
        "| samtools sort -@ {threads} -m 5G -o {output}) 2> {log}"

# Filter alignments for mismatches
rule samtools_lowXM:
    input:
        "mapped/{sample}_MappedOn_{refbase}_sort.bam"
    output:
        lowXMSAM = "mapped/{sample}_MappedOn_{refbase}_sort_lowXM.sam",
        headSAM  = "mapped/{sample}_MappedOn_{refbase}_sort_header.sam"
    log:
        "logs/samtools/{sample}_MappedOn_{refbase}_sort_lowXM.log"
    shell:
        # Allow a maximum of 6 mismatches
        # ([^0-9] matches characters not in the range of 0 to 9)
        # http://seqanswers.com/forums/showthread.php?t=19729
        "(samtools view -Sh {input} |" 
        " grep -e \"^@\" -e \"XM:i:[0-6][^0-9]\" > {output.lowXMSAM}; "
        "samtools view -H {output.lowXMSAM} > {output.headSAM}) 2> {log}"
 
# Extract unique alignments
rule samtools_unique:
    input:
        lowXMSAM = "mapped/{sample}_MappedOn_{refbase}_sort_lowXM.sam",
        headSAM  = "mapped/{sample}_MappedOn_{refbase}_sort_header.sam"
    output:
        uniqSAM  = "mapped/{sample}_MappedOn_{refbase}_sort_lowXM_unique.sam",
        multSAM  = "mapped/{sample}_MappedOn_{refbase}_sort_lowXM_multi.sam",
        bothSAM  = "mapped/{sample}_MappedOn_{refbase}_sort_lowXM_both.sam"
    log:
        "logs/samtools/{sample}_MappedOn_{refbase}_sort_dedup_filtered.log"
    threads: config["THREADS"]
    shell:
        # Allow a maximum of 6 mismatches
        # ([^0-9] matches characters not in the range of 0 to 9)
        # http://seqanswers.com/forums/showthread.php?t=19729
        "(samtools view -Sh {input} |" 
        " grep -e \"^@\" -e \"XM:i:[0-6][^0-9]\" > {output.lowXMSAM}; "
        "samtools view -H {output.lowXMSAM} > {output.headSAM}) 2> {log}"
        # Extract unique alignments
        # http://biofinysics.blogspot.com/2014/05/how-does-bowtie2-assign-mapq-scores.html
        #https://sequencing.qcfail.com/articles/mapq-values-are-really-useful-but-their-implementation-is-a-mess/
        "samtools view -Sh {output.lowXMSAM} | grep -v \"XS:i:\" > {output.uniqSAM}; "


# Postmapping steps:
# Index BAM file (index format [bai or csi] depends on chromosome sizes)
# Generate samtools flagstat and idxstats
# Calculate library-size-normalized coverage
if bamidx == "bai":
    rule postmapping:
        """bam.bai samtools flagstat idxstats"""
        input:
            "mapped/{sample}_MappedOn_{refbase}_sort.bam"
        output:
            "mapped/{sample}_MappedOn_{refbase}_sort.bam.{bamidx}"
        log:
            flagstat = "logs/bowtie/{sample}_MappedOn_{refbase}_sort_flagstat.log",
            idxstats = "logs/bowtie/{sample}_MappedOn_{refbase}_sort_idxstats.log"
        shell:
            """
            samtools index    {input}
            samtools flagstat {input} > {log.flagstat}
            samtools idxstats {input} > {log.idxstats}
            """
    rule calc_coverage:
        """Calculate library-size-normalized coverage"""
        input:
            bam    = "mapped/{sample}_MappedOn_{refbase}_sort.bam",
            bamidx = "mapped/{sample}_MappedOn_{refbase}_sort.bam.bai"
        output:
            bw = "mapped/bw/{sample}_MappedOn_{refbase}_sort_norm.bw",
            bg = "mapped/bg/{sample}_MappedOn_{refbase}_sort_norm.bedgraph"
        params:
            normalizeUsing         = config["COVERAGE"]["normalizeUsing"],
            ignoreForNormalization = config["COVERAGE"]["ignoreForNormalization"],
            binSize                = config["COVERAGE"]["binSize"]
        log:
            "logs/bamCoverage/{sample}_MappedOn_{refbase}_sort_norm.log"
        threads: config["THREADS"]  
        shell:
            "bamCoverage -b {input.bam} -o {output.bw}"
            " --normalizeUsing {params.normalizeUsing}"
            " --ignoreForNormalization {params.ignoreForNormalization}"
            " --binSize {params.binSize} -p {threads}; "
            "bamCoverage -b {input.bam} -o {output.bg} -of bedgraph"
            " --normalizeUsing {params.normalizeUsing}"
            " --ignoreForNormalization {params.ignoreForNormalization}"
            " --binSize {params.binSize} -p {threads}"
else:
    rule postmapping:
        """bam.csi samtools flagstat idxstats"""
        input:
            "mapped/{sample}_MappedOn_{refbase}_sort.bam"
        output:
            "mapped/{sample}_MappedOn_{refbase}_sort.bam.{bamidx}"
        log:
            flagstat = "logs/bowtie/{sample}_MappedOn_{refbase}_sort_flagstat.log",
            idxstats = "logs/bowtie/{sample}_MappedOn_{refbase}_sort_idxstats.log"
        shell:
            """
            samtools index -c -m 14 {input}
            samtools flagstat       {input} > {log.flagstat}
            samtools idxstats       {input} > {log.idxstats}
            """
    rule calc_coverage:
        """Calculate library-size-normalized coverage"""
        input:
            bam    = "mapped/{sample}_MappedOn_{refbase}_sort.bam",
            bamidx = "mapped/{sample}_MappedOn_{refbase}_sort.bam.csi"
        output:
            bw = "mapped/bw/{sample}_MappedOn_{refbase}_sort_norm.bw",
            bg = "mapped/bg/{sample}_MappedOn_{refbase}_sort_norm.bedgraph"
        params:
            normalizeUsing         = config["COVERAGE"]["normalizeUsing"],
            ignoreForNormalization = config["COVERAGE"]["ignoreForNormalization"],
            binSize                = config["COVERAGE"]["binSize"]
        log:
            "logs/bamCoverage/{sample}_MappedOn_{refbase}_sort_norm.log"
        threads: config["THREADS"]  
        shell:
            "bamCoverage -b {input.bam} -o {output.bw}"
            " --normalizeUsing {params.normalizeUsing}"
            " --ignoreForNormalization {params.ignoreForNormalization}"
            " --binSize {params.binSize} -p {threads}; "
            "bamCoverage -b {input.bam} -o {output.bg} -of bedgraph"
            " --normalizeUsing {params.normalizeUsing}"
            " --ignoreForNormalization {params.ignoreForNormalization}"
            " --binSize {params.binSize} -p {threads}"

# Convert bedgraph to per-base 1-based coverage file
rule per_base_coverage:
    """Convert bedgraph to per-base 1-based coverage file"""
    input:
        "mapped/bg/{sample}_MappedOn_{refbase}_sort_norm.bedgraph"
    output:
        "mapped/pb/{sample}_MappedOn_{refbase}_sort_norm.perbase"
    log:
        "logs/perBaseCoverage/{sample}_MappedOn_{refbase}_sort_norm_pb.log"
    shell:
        # Note, Snakemake doesn't support shell scripts as "scripts"
        # Using the "shell" directive instead
        "bash {SRCdIR}/scripts/perbase_1based_coverage.sh {input} {output}"
